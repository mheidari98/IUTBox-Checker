{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cloudy-attendance",
   "metadata": {},
   "source": [
    "pip install requests  \n",
    "\n",
    "pip install lxml  \n",
    "pip install html5lib\n",
    "\n",
    "pip install beautifulsoup4      \n",
    "pip install bs4    \n",
    "\n",
    "pip install -U selenium   \n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "western-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assigned-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://iutbox.iut.ac.ir/index.php/s/MtzkTTDnHrfte3R?path=%2FVideo'\n",
    "url = 'https://iutbox.iut.ac.ir/index.php/s/MtzkTTDnHrfte3R'\n",
    "\n",
    "MyPATH = Path('../FDM2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-crown",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-amino",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eastern-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newPath(url, newDir):\n",
    "    \"\"\"generate newURL and DownloadURL based on public link and directory\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            Base url\n",
    "        newDir : str\n",
    "            directory that want to join with base url\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_url, path = url.split('?path=')\n",
    "    except:\n",
    "        base_url = url.split('?path=')[0]\n",
    "        path = ''  #'%2F'\n",
    "    \n",
    "    newPath = path + ( '' if newDir=='' else ('%2F' + newDir) )\n",
    "    \n",
    "    if newPath == '':\n",
    "        newPath = '%2F'\n",
    "    \n",
    "    newURL = base_url + '?path=' + newPath\n",
    "    DownloadURL = base_url + \"/download\" + '?path=' + newPath + \"&files=\"\n",
    "    \n",
    "    return newURL, DownloadURL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hydraulic-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recPath(url, args):\n",
    "    for item in args:\n",
    "        url = newPath(url, item)[0]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-arrangement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "synthetic-noise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://iutbox.iut.ac.ir/index.php/s/MtzkTTDnHrfte3R?path=%2F',\n",
       " 'https://iutbox.iut.ac.ir/index.php/s/MtzkTTDnHrfte3R/download?path=%2F&files=')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url , downlaod_link = newPath(url, '')\n",
    "url , downlaod_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-creation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recorded-enterprise",
   "metadata": {},
   "source": [
    "## Static Websites\n",
    "https://realpython.com/beautiful-soup-web-scraper-python/#dynamic-websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "signal-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get(url, allow_redirects=True)\n",
    "# html_doc = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-recommendation",
   "metadata": {},
   "source": [
    "## Dynamic Websites\n",
    "https://realpython.com/modern-web-automation-with-python-and-selenium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "whole-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_webdriver(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument(\"--enable-javascript\")\n",
    "    options.headless = True\n",
    "\n",
    "    driver = webdriver.Chrome('./chromedriver.exe', options=options) # geckodriver.exe\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.get(url) \n",
    "\n",
    "    # this is just to ensure that the page is loaded\n",
    "    time.sleep(5) \n",
    "    \n",
    "    \n",
    "    #### For Scroll infinite in page    \n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "    html_doc = driver.page_source\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')  # 'html5lib'\n",
    "    # print(soup.prettify())\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comic-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "\n",
    "def SimplePrint(num, Type, name , size, modified_original ):\n",
    "    color = bcolors.OKGREEN if Type==1 else bcolors.FAIL\n",
    "    print(f\"{color}{num:02d}) {name}\" + \" \"*((7*10 -(len(name)+3) )) + f\"({size})  \\t({modified_original}){bcolors.ENDC}\")\n",
    "    \n",
    "def TreePrint(lvl, Type, name ):\n",
    "    color = bcolors.OKGREEN if Type==1 else bcolors.FAIL\n",
    "    print(\"\\t\"*lvl + f\"{color}{name}{bcolors.ENDC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "certified-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsl(url=None):\n",
    "    if url==None:\n",
    "        print(\"Please pass URL\")\n",
    "        return\n",
    "    \n",
    "    soup = initialize_webdriver(url)\n",
    "    \n",
    "    result = soup.find_all('span', class_='nametext') # driver.find_elements_by_class_name('innernametext') #\n",
    "    filesize = soup.find_all('td', class_='filesize')\n",
    "    modified_data = soup.find_all('span', class_='modified live-relative-timestamp')\n",
    "\n",
    "    for num, item in enumerate(result, start=0):\n",
    "        innernametext =  item.find('span', class_='innernametext').text\n",
    "        extension =  item.find('span', class_='extension')\n",
    "        size = filesize[num].text\n",
    "        modified_relative = modified_data[num].text\n",
    "        modified_original = modified_data[num].get('data-original-title')\n",
    "\n",
    "        if extension == None :\n",
    "            DirName = innernametext\n",
    "            SimplePrint(num, 1, DirName , size, modified_original)\n",
    "        else :\n",
    "            filename = innernametext + extension.text\n",
    "            SimplePrint(num, 0, filename , size, modified_original) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "composed-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m00) Datasets                                                           (۳٫۷ MB)  \t(۲۸ فوریه ۲۰۲۱ ۲۰:۵۸)\u001b[0m\n",
      "\u001b[92m01) Extra                                                              (۴۸٫۶ MB)  \t(۲۶ فوریه ۲۰۲۱ ۱۰:۵۲)\u001b[0m\n",
      "\u001b[92m02) Ref                                                                (۴۲٫۵ MB)  \t(۳۱ مه ۲۰۲۱ ۱۸:۳۰)\u001b[0m\n",
      "\u001b[92m03) Video                                                              (۱٫۵ GB)  \t(۳۱ مه ۲۰۲۱ ۱۹:۳۱)\u001b[0m\n",
      "\u001b[91m04) FDM2021-Class-Rules-V12.pdf                                        (268 KB)  \t(۱۲ فوریه ۲۰۲۱ ۲۲:۳۹)\u001b[0m\n",
      "\u001b[91m05) FDM2021-Class-Rules-V14.pdf                                        (269 KB)  \t(۲۳ فوریه ۲۰۲۱ ۱۰:۴۰)\u001b[0m\n",
      "\u001b[91m06) FDM2021-Class-Rules-V15.pdf                                        (299 KB)  \t(۳۰ آوریل ۲۰۲۱ ۰۵:۳۷)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lsl( newPath(url, '')[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compact-argument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m00) BookParts                                                          (864 KB)  \t(۲۱ مه ۲۰۲۱ ۱۲:۳۵)\u001b[0m\n",
      "\u001b[92m01) Notebook                                                           (16 KB)  \t(۱۲ آوریل ۲۰۲۱ ۱۳:۱۴)\u001b[0m\n",
      "\u001b[92m02) Slides                                                             (۴٫۹ MB)  \t(۳۱ مه ۲۰۲۱ ۱۸:۲۹)\u001b[0m\n",
      "\u001b[91m03) BK1-IntroDM.pdf                                                    (۲۶٫۴ MB)  \t(۵ فوریه ۲۰۲۱ ۲۰:۰۰)\u001b[0m\n",
      "\u001b[91m04) BK2-DSPR.pdf                                                       (۴٫۷ MB)  \t(۷ دسامبر ۲۰۲۰ ۱۸:۱۴)\u001b[0m\n",
      "\u001b[91m05) BK3-DKID2nd.pdf                                                    (۵٫۷ MB)  \t(۱۲ نوامبر ۲۰۱۴ ۰۱:۵۲)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lsl( newPath(url, 'Ref')[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-motion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "searching-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(url=None, ShowFiles=0, lvl=0):\n",
    "    if url==None:\n",
    "        print(\"Please pass URL\")\n",
    "        return\n",
    "    \n",
    "    soup = initialize_webdriver(url)\n",
    "    \n",
    "    result = soup.find_all('span', class_='nametext') # driver.find_elements_by_class_name('innernametext') #\n",
    "    filesize = soup.find_all('td', class_='filesize')\n",
    "    modified_data = soup.find_all('span', class_='modified live-relative-timestamp')\n",
    "\n",
    "    for num, item in enumerate(result, start=0):\n",
    "        innernametext =  item.find('span', class_='innernametext').text\n",
    "        extension =  item.find('span', class_='extension')\n",
    "        size = filesize[num].text\n",
    "        modified_relative = modified_data[num].text\n",
    "        modified_original = modified_data[num].get('data-original-title')\n",
    "\n",
    "        if extension == None :\n",
    "            DirName = innernametext\n",
    "            TreePrint(lvl, 1, DirName)\n",
    "            tree( newPath(url, DirName)[0] , ShowFiles, lvl+1)\n",
    "        elif ShowFiles :\n",
    "            filename = innernametext + extension.text\n",
    "            TreePrint(lvl, 0, filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "political-northwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mDatasets\u001b[0m\n",
      "\t\u001b[91mBK1-IntroDM-Datasets.zip\u001b[0m\n",
      "\t\u001b[91mBK2-DSPR-Datasets.zip\u001b[0m\n",
      "\t\u001b[91mBK3-DKID2e-Datasets.zip\u001b[0m\n",
      "\u001b[92mExtra\u001b[0m\n",
      "\t\u001b[91mW02-Extra-Pandas-Samani-nr.mp4\u001b[0m\n",
      "\u001b[92mRef\u001b[0m\n",
      "\t\u001b[92mBookParts\u001b[0m\n",
      "\t\t\u001b[91mBK1-AssociationAnalysis-text.pdf\u001b[0m\n",
      "\t\t\u001b[91mNaive-Bayes-from-BK1-IntroDM-orig-marked.pdf\u001b[0m\n",
      "\t\t\u001b[91mNaiveBayes-from-Vishnu Pendyala - Veracity of Big Data.pdf\u001b[0m\n",
      "\t\u001b[92mNotebook\u001b[0m\n",
      "\t\t\u001b[91mNaiveBayes-and-Eval-BK2.Ch8and7.ipynb\u001b[0m\n",
      "\t\u001b[92mSlides\u001b[0m\n",
      "\t\t\u001b[91mBK1-Chapter2-data-sel1.pdf\u001b[0m\n",
      "\t\t\u001b[91mBK1-Chapter5_basic_association_analysis-sel.pdf\u001b[0m\n",
      "\t\t\u001b[91mBK1-Chapter6_advanced_association_analysis-sel.pdf\u001b[0m\n",
      "\t\t\u001b[91mBK1-Chapter7_basic_cluster_analysis-sel.pdf\u001b[0m\n",
      "\t\t\u001b[91mBK3-Chapter 1.pdf\u001b[0m\n",
      "\t\t\u001b[91mBK3-Chapter 2-sel.pdf\u001b[0m\n",
      "\t\u001b[91mBK1-IntroDM.pdf\u001b[0m\n",
      "\t\u001b[91mBK2-DSPR.pdf\u001b[0m\n",
      "\t\u001b[91mBK3-DKID2nd.pdf\u001b[0m\n",
      "\u001b[92mVideo\u001b[0m\n",
      "\t\u001b[91mW01-0-Course-Rules.mp4\u001b[0m\n",
      "\t\u001b[91mW01-1-DM-Intro-Process.mp4\u001b[0m\n",
      "\t\u001b[91mW02-1-DM-Intro-BK1.mp4\u001b[0m\n",
      "\t\u001b[91mW02-Extra-link-to-basics.mp4\u001b[0m\n",
      "\t\u001b[91mW03-1-part1-Data Preprocessing cont, (BK3.Ch2).mp4\u001b[0m\n",
      "\t\u001b[91mW03-1-part2-Problem Understanding (BK2.ch3.part1).mp4\u001b[0m\n",
      "\t\u001b[91mW03-1-part3-Data Preprocessing py (BK2.ch3.part2).mp4\u001b[0m\n",
      "\t\u001b[91mW04-1-part1-EDA (BK3.Ch3).mp4\u001b[0m\n",
      "\t\u001b[91mW04-1-part2-EDA (BK2.Ch4).mp4\u001b[0m\n",
      "\t\u001b[91mW05-1-Pre-Model (BK2.Ch5).mp4\u001b[0m\n",
      "\t\u001b[91mW05-2-Model-DeciTree-CART (BK2.Ch6).mp4\u001b[0m\n",
      "\t\u001b[91mW06-1-Data(BK1.Ch2.part1).mp4\u001b[0m\n",
      "\t\u001b[91mW06-1-Data(BK1.Ch2.part2).mp4\u001b[0m\n",
      "\t\u001b[91mW06-1-Data(BK1.Ch2.part3).mp4\u001b[0m\n",
      "\t\u001b[91mW06-2-Classification-DT-Gini(BK1.Ch3.part1).mp4\u001b[0m\n",
      "\t\u001b[91mW06-2-Classification-DT-X-Entropy(BK1.Ch3.part2).mp4\u001b[0m\n",
      "\t\u001b[91mW07-1-RandomForest(BK2.Ch6.and.PyDSHB).mp4\u001b[0m\n",
      "\t\u001b[91mW07-2-Evaluation(BK2.Ch7.PyDSHB).mp4\u001b[0m\n",
      "\t\u001b[91mW07-3-NaiveBayes(BK1.Ch4-BK2.Ch8-and-PyDSHB).mp4\u001b[0m\n",
      "\t\u001b[91mW08-NeuralNetworks(BK2.Ch9, BK1.Ch4, site).mp4\u001b[0m\n",
      "\t\u001b[91mW09-1-Clustering(BK2.Ch10).mp4\u001b[0m\n",
      "\t\u001b[91mW09-2-Clustering(BK1.Ch7a).mp4\u001b[0m\n",
      "\t\u001b[91mW09-3-Clustering-Kmeans-ext(BK1.Ch7b, scikit).mp4\u001b[0m\n",
      "\t\u001b[91mW09-4-Clustering-Hierarchical(BK1.Ch7c, scikit).mp4\u001b[0m\n",
      "\t\u001b[91mW11-1-Clustering-DensityBased(BK1.Ch7d, scikit).mp4\u001b[0m\n",
      "\t\u001b[91mW11-2-Clustering-Validity(BK1.Ch7e, scikit).mp4\u001b[0m\n",
      "\t\u001b[91mW11-3-Regression(BK2.Ch11, statmodels).mp4\u001b[0m\n",
      "\t\u001b[91mW12-1-DimensionReduction(BK2.Ch12, PDSH).mp4\u001b[0m\n",
      "\t\u001b[91mW13-1-AssociationRules-basic(BK1.Ch5, PBPy).mp4\u001b[0m\n",
      "\t\u001b[91mW14-1-AdvRegression(Bk2.Ch13, sklrn).mp4\u001b[0m\n",
      "\t\u001b[91mW14-2-AdvAssociationRules(Bk1.Ch6).mp4\u001b[0m\n",
      "\t\u001b[91mW14-3-SequenceMining(Bk1.Ch6).mp4\u001b[0m\n",
      "\u001b[91mFDM2021-Class-Rules-V12.pdf\u001b[0m\n",
      "\u001b[91mFDM2021-Class-Rules-V14.pdf\u001b[0m\n",
      "\u001b[91mFDM2021-Class-Rules-V15.pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tree( newPath(url, '')[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-basket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "running-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckExist(PathList, Name, lvl): \n",
    "    tmp = MyPATH\n",
    "    for i in range(lvl+1):\n",
    "        tmp = tmp.joinpath( PathList[i] )\n",
    "    tmp = tmp.joinpath( Name ) \n",
    "    \n",
    "    tmpFlag = tmp.exists()\n",
    "    \n",
    "    if tmpFlag :\n",
    "        print(\"✅\", tmp)\n",
    "    else:\n",
    "        print(\"❌\", tmp)\n",
    "        \n",
    "    return tmpFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "legislative-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullCheck(url=None, PathList=[], ShowFiles=0, lvl=0):\n",
    "    if url==None:\n",
    "        print(\"Please pass URL\")\n",
    "        return\n",
    "    \n",
    "    url = recPath(url, PathList )\n",
    "    \n",
    "    soup = initialize_webdriver(url)\n",
    "    \n",
    "    result = soup.find_all('span', class_='nametext') # driver.find_elements_by_class_name('innernametext') #\n",
    "    filesize = soup.find_all('td', class_='filesize')\n",
    "    modified_data = soup.find_all('span', class_='modified live-relative-timestamp')\n",
    "\n",
    "    for num, item in enumerate(result, start=0):\n",
    "        innernametext =  item.find('span', class_='innernametext').text\n",
    "        extension =  item.find('span', class_='extension')\n",
    "        size = filesize[num].text\n",
    "        modified_relative = modified_data[num].text\n",
    "        modified_original = modified_data[num].get('data-original-title')\n",
    "\n",
    "        if extension == None :\n",
    "            DirName = innernametext\n",
    "#             TreePrint(lvl, 1, DirName)\n",
    "            \n",
    "            if not CheckExist(PathList, DirName, lvl) :\n",
    "                continue\n",
    "                \n",
    "            FullCheck( url , PathList+[DirName], ShowFiles, lvl+1)\n",
    "        elif ShowFiles :\n",
    "            filename = innernametext + extension.text\n",
    "#             TreePrint(lvl, 0, filename) \n",
    "            \n",
    "            if not CheckExist(PathList, filename, lvl) :\n",
    "                continue\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "strange-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ..\\FDM2021\\Datasets\n",
      "✅ ..\\FDM2021\\Datasets\\BK1-IntroDM-Datasets.zip\n",
      "✅ ..\\FDM2021\\Datasets\\BK2-DSPR-Datasets.zip\n",
      "✅ ..\\FDM2021\\Datasets\\BK3-DKID2e-Datasets.zip\n",
      "✅ ..\\FDM2021\\Extra\n",
      "✅ ..\\FDM2021\\Extra\\W02-Extra-Pandas-Samani-nr.mp4\n",
      "✅ ..\\FDM2021\\Ref\n",
      "✅ ..\\FDM2021\\Ref\\BookParts\n",
      "✅ ..\\FDM2021\\Ref\\Notebook\n",
      "✅ ..\\FDM2021\\Ref\\Slides\n",
      "✅ ..\\FDM2021\\Ref\\BK1-IntroDM.pdf\n",
      "✅ ..\\FDM2021\\Ref\\BK2-DSPR.pdf\n",
      "✅ ..\\FDM2021\\Ref\\BK3-DKID2nd.pdf\n",
      "✅ ..\\FDM2021\\Video\n",
      "✅ ..\\FDM2021\\Video\\W01-0-Course-Rules.mp4\n",
      "✅ ..\\FDM2021\\Video\\W01-1-DM-Intro-Process.mp4\n",
      "✅ ..\\FDM2021\\Video\\W02-1-DM-Intro-BK1.mp4\n",
      "❌ ..\\FDM2021\\Video\\W02-Extra-link-to-basics.mp4\n",
      "✅ ..\\FDM2021\\Video\\W03-1-part1-Data Preprocessing cont, (BK3.Ch2).mp4\n",
      "✅ ..\\FDM2021\\Video\\W03-1-part2-Problem Understanding (BK2.ch3.part1).mp4\n",
      "✅ ..\\FDM2021\\Video\\W03-1-part3-Data Preprocessing py (BK2.ch3.part2).mp4\n",
      "✅ ..\\FDM2021\\Video\\W04-1-part1-EDA (BK3.Ch3).mp4\n",
      "✅ ..\\FDM2021\\Video\\W04-1-part2-EDA (BK2.Ch4).mp4\n",
      "✅ ..\\FDM2021\\Video\\W05-1-Pre-Model (BK2.Ch5).mp4\n",
      "✅ ..\\FDM2021\\Video\\W05-2-Model-DeciTree-CART (BK2.Ch6).mp4\n",
      "✅ ..\\FDM2021\\Video\\W06-1-Data(BK1.Ch2.part1).mp4\n",
      "✅ ..\\FDM2021\\Video\\W06-1-Data(BK1.Ch2.part2).mp4\n",
      "✅ ..\\FDM2021\\Video\\W06-1-Data(BK1.Ch2.part3).mp4\n",
      "✅ ..\\FDM2021\\Video\\W06-2-Classification-DT-Gini(BK1.Ch3.part1).mp4\n",
      "✅ ..\\FDM2021\\Video\\W06-2-Classification-DT-X-Entropy(BK1.Ch3.part2).mp4\n",
      "✅ ..\\FDM2021\\Video\\W07-1-RandomForest(BK2.Ch6.and.PyDSHB).mp4\n",
      "✅ ..\\FDM2021\\Video\\W07-2-Evaluation(BK2.Ch7.PyDSHB).mp4\n",
      "✅ ..\\FDM2021\\Video\\W07-3-NaiveBayes(BK1.Ch4-BK2.Ch8-and-PyDSHB).mp4\n",
      "❌ ..\\FDM2021\\Video\\W08-NeuralNetworks(BK2.Ch9, BK1.Ch4, site).mp4\n",
      "✅ ..\\FDM2021\\Video\\W09-1-Clustering(BK2.Ch10).mp4\n",
      "✅ ..\\FDM2021\\Video\\W09-2-Clustering(BK1.Ch7a).mp4\n",
      "✅ ..\\FDM2021\\Video\\W09-3-Clustering-Kmeans-ext(BK1.Ch7b, scikit).mp4\n",
      "✅ ..\\FDM2021\\Video\\W09-4-Clustering-Hierarchical(BK1.Ch7c, scikit).mp4\n",
      "✅ ..\\FDM2021\\Video\\W11-1-Clustering-DensityBased(BK1.Ch7d, scikit).mp4\n",
      "✅ ..\\FDM2021\\Video\\W11-2-Clustering-Validity(BK1.Ch7e, scikit).mp4\n",
      "✅ ..\\FDM2021\\Video\\W11-3-Regression(BK2.Ch11, statmodels).mp4\n",
      "✅ ..\\FDM2021\\Video\\W12-1-DimensionReduction(BK2.Ch12, PDSH).mp4\n",
      "✅ ..\\FDM2021\\Video\\W13-1-AssociationRules-basic(BK1.Ch5, PBPy).mp4\n",
      "✅ ..\\FDM2021\\Video\\W14-1-AdvRegression(Bk2.Ch13, sklrn).mp4\n",
      "✅ ..\\FDM2021\\Video\\W14-2-AdvAssociationRules(Bk1.Ch6).mp4\n",
      "✅ ..\\FDM2021\\Video\\W14-3-SequenceMining(Bk1.Ch6).mp4\n",
      "✅ ..\\FDM2021\\FDM2021-Class-Rules-V12.pdf\n",
      "✅ ..\\FDM2021\\FDM2021-Class-Rules-V14.pdf\n",
      "❌ ..\\FDM2021\\FDM2021-Class-Rules-V15.pdf\n"
     ]
    }
   ],
   "source": [
    "FullCheck( url, [''], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-express",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-correlation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-plasma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "legendary-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib.request.urlretrieve(download_list[31], filename_list[31]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-converter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-fancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-steering",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
